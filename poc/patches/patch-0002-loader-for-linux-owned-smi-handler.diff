From b43ae0f41b7338a7ff7b7e032304e0482aaac955 Mon Sep 17 00:00:00 2001
From: Michal Gorlas <michal.gorlas@9elements.com>
Date: Mon, 28 Apr 2025 15:34:02 +0200
Subject: [PATCH] [PATCH 1/3] loader for Linux-owned SMI handler

placeholder for now

Signed-off-by: Michal Gorlas <michal.gorlas@9elements.com>
---
 arch/x86/include/asm/realmode.h      |  16 +-
 arch/x86/kernel/head_64.S            |  22 +++
 arch/x86/realmode/init.c             |  56 ++++--
 arch/x86/realmode/rm/Makefile        |   1 +
 arch/x86/realmode/rm/header.S        |   8 +-
 arch/x86/realmode/rm/mm_trampoline.S | 242 ++++++++++++++++++++++++++
 drivers/firmware/google/Makefile     |   1 +
 drivers/firmware/google/mm_loader.c  | 247 +++++++++++++++++++++++++++
 drivers/firmware/google/smm.h        |   7 +
 9 files changed, 585 insertions(+), 15 deletions(-)
 create mode 100644 arch/x86/realmode/rm/mm_trampoline.S
 create mode 100644 drivers/firmware/google/mm_loader.c

diff --git a/arch/x86/include/asm/realmode.h b/arch/x86/include/asm/realmode.h
index f607081a022a..af9499f2df44 100644
--- a/arch/x86/include/asm/realmode.h
+++ b/arch/x86/include/asm/realmode.h
@@ -9,7 +9,9 @@
 #define TH_FLAGS_SME_ACTIVE_BIT		0
 #define TH_FLAGS_SME_ACTIVE		BIT(TH_FLAGS_SME_ACTIVE_BIT)
 
-#ifndef __ASSEMBLER__
+#define SMM_INIT_HANDLER 1
+
+#ifndef __ASSEMBLY__
 
 #include <linux/types.h>
 #include <asm/io.h>
@@ -21,6 +23,13 @@ struct real_mode_header {
 	/* SMP trampoline */
 	u32	trampoline_start;
 	u32	trampoline_header;
+#ifdef CONFIG_SMM_DRIVER
+	/* Needed only for MM payload */
+	u32	mm_startup_32;
+	u32	mm_trampoline_header;
+	u32	mm_trampoline_start64;
+	u32	mm_trampoline_pgd;
+#endif
 #ifdef CONFIG_AMD_MEM_ENCRYPT
 	u32	sev_es_trampoline_start;
 #endif
@@ -61,6 +70,7 @@ extern unsigned char real_mode_blob_end[];
 
 extern unsigned long initial_code;
 extern unsigned long initial_stack;
+extern unsigned long ending_code;
 #ifdef CONFIG_AMD_MEM_ENCRYPT
 extern unsigned long initial_vc_handler;
 #endif
@@ -77,6 +87,8 @@ extern unsigned char boot_gdt[];
 extern unsigned char secondary_startup_64[];
 extern unsigned char secondary_startup_64_no_verify[];
 #endif
+extern struct trampoline_header *trampoline_header;
+extern unsigned int is_for_smm;
 
 static inline size_t real_mode_size_needed(void)
 {
@@ -88,7 +100,7 @@ static inline size_t real_mode_size_needed(void)
 
 static inline void set_real_mode_mem(phys_addr_t mem)
 {
-	real_mode_header = (struct real_mode_header *) __va(mem);
+	real_mode_header = (struct real_mode_header *)__va(mem);
 }
 
 void reserve_real_mode(void);
diff --git a/arch/x86/kernel/head_64.S b/arch/x86/kernel/head_64.S
index fefe2a25cf02..d2e607d0bbd2 100644
--- a/arch/x86/kernel/head_64.S
+++ b/arch/x86/kernel/head_64.S
@@ -27,6 +27,7 @@
 #include <asm/fixmap.h>
 #include <asm/smp.h>
 #include <asm/thread_info.h>
+#include <asm/realmode.h>
 
 /*
  * We are not able to switch in one step to the final KERNEL ADDRESS SPACE
@@ -101,6 +102,7 @@ SYM_CODE_START_NOALIGN(startup_64)
 	 * Derive the kernel's physical-to-virtual offset from the physical and
 	 * virtual addresses of common_startup_64().
 	 */
+
 	leaq	common_startup_64(%rip), %rdi
 	subq	.Lcommon_startup_64(%rip), %rdi
 
@@ -412,11 +414,28 @@ SYM_INNER_LABEL(common_startup_64, SYM_L_LOCAL)
 	/* Pass the boot_params pointer as first argument */
 	movq	%r15, %rdi
 
+	/* Only if is_for_smm was modified (only done if mm_loader is compiled),
+	 * we jump to the SMI handler instead of initial boot code. This is
+	 * safe as is_for_smm is set to 0 by realmode/init.c which is one of the
+	 * first things being run. Now, by the time the drivers (and so mm_loader)
+	 * are executed, we do not need the setup code anywhere else. Thus then
+	 * changing the is_for_smm won't have any fatal consequences for the boot.
+	 */
+	movl	is_for_smm(%rip), %ecx
+	testl	$SMM_INIT_HANDLER, %ecx
+	jnz	.Ljump_to_C_handler
+
 .Ljump_to_C_code:
 	xorl	%ebp, %ebp	# clear frame pointer
 	ANNOTATE_RETPOLINE_SAFE
 	callq	*initial_code(%rip)
 	ud2
+
+.Ljump_to_C_handler:
+	xorl	%ebp, %ebp
+	ANNOTATE_RETPOLINE_SAFE
+	callq	*ending_code(%rip)
+
 SYM_CODE_END(secondary_startup_64)
 
 #include "verify_cpu.S"
@@ -480,6 +499,8 @@ SYM_DATA(initial_code,	.quad x86_64_start_kernel)
 #ifdef CONFIG_AMD_MEM_ENCRYPT
 SYM_DATA(initial_vc_handler,	.quad handle_vc_boot_ghcb)
 #endif
+/*  */
+SYM_DATA(ending_code,	.quad 0)
 
 SYM_DATA(trampoline_lock, .quad 0);
 	__FINITDATA
@@ -699,6 +720,7 @@ SYM_DATA_END(level1_fixmap_pgt)
 	.align 16
 
 SYM_DATA(smpboot_control,		.long 0)
+SYM_DATA(is_for_smm,			.long 0)
 
 	.align 16
 /* This must match the first entry in level2_kernel_pgt */
diff --git a/arch/x86/realmode/init.c b/arch/x86/realmode/init.c
index f9bc444a3064..ddef7f0bbc79 100644
--- a/arch/x86/realmode/init.c
+++ b/arch/x86/realmode/init.c
@@ -78,7 +78,7 @@ static void __init sme_sev_setup_real_mode(struct trampoline_header *th)
 		 * Skip the call to verify_cpu() in secondary_startup_64 as it
 		 * will cause #VC exceptions when the AP can't handle them yet.
 		 */
-		th->start = (u64) secondary_startup_64_no_verify;
+		th->start = (u64)secondary_startup_64_no_verify;
 
 		if (sev_es_setup_ap_jump_table(real_mode_header))
 			panic("Failed to get/update SEV-ES AP Jump Table");
@@ -86,6 +86,10 @@ static void __init sme_sev_setup_real_mode(struct trampoline_header *th)
 #endif
 }
 
+struct trampoline_header *trampoline_header;
+#ifdef CONFIG_SMM_DRIVER
+struct trampoline_header *mm_tram_header;
+#endif
 static void __init setup_real_mode(void)
 {
 	u16 real_mode_seg;
@@ -93,7 +97,6 @@ static void __init setup_real_mode(void)
 	u32 count;
 	unsigned char *base;
 	unsigned long phys_base;
-	struct trampoline_header *trampoline_header;
 	size_t size = PAGE_ALIGN(real_mode_blob_end - real_mode_blob);
 #ifdef CONFIG_X86_64
 	u64 *trampoline_pgd;
@@ -116,19 +119,19 @@ static void __init setup_real_mode(void)
 	phys_base = __pa(base);
 	real_mode_seg = phys_base >> 4;
 
-	rel = (u32 *) real_mode_relocs;
+	rel = (u32 *)real_mode_relocs;
 
 	/* 16-bit segment relocations. */
 	count = *rel++;
 	while (count--) {
-		u16 *seg = (u16 *) (base + *rel++);
+		u16 *seg = (u16 *)(base + *rel++);
 		*seg = real_mode_seg;
 	}
 
 	/* 32-bit linear relocations. */
 	count = *rel++;
 	while (count--) {
-		u32 *ptr = (u32 *) (base + *rel++);
+		u32 *ptr = (u32 *)(base + *rel++);
 		*ptr += phys_base;
 	}
 
@@ -148,7 +151,8 @@ static void __init setup_real_mode(void)
 	rdmsrl(MSR_EFER, efer);
 	trampoline_header->efer = efer & ~EFER_LMA;
 
-	trampoline_header->start = (u64) secondary_startup_64;
+	trampoline_header->start = (u64)secondary_startup_64;
+
 	trampoline_cr4_features = &trampoline_header->cr4;
 	*trampoline_cr4_features = mmu_cr4_features;
 
@@ -157,7 +161,7 @@ static void __init setup_real_mode(void)
 	trampoline_lock = &trampoline_header->lock;
 	*trampoline_lock = 0;
 
-	trampoline_pgd = (u64 *) __va(real_mode_header->trampoline_pgd);
+	trampoline_pgd = (u64 *)__va(real_mode_header->trampoline_pgd);
 
 	/* Map the real mode stub as virtual == physical */
 	trampoline_pgd[0] = trampoline_pgd_entry.pgd;
@@ -171,6 +175,34 @@ static void __init setup_real_mode(void)
 		trampoline_pgd[i] = init_top_pgt[i].pgd;
 #endif
 
+#ifdef CONFIG_SMM_DRIVER
+	// Not needed for now
+	//u64 *mm_trampoline_pgd;
+	//u32 *mm_tram_cr4_features;
+
+	mm_tram_header =  (struct trampoline_header *)
+		__va(real_mode_header->trampoline_header);
+	mm_tram_header->efer = efer & ~EFER_LMA;
+
+	mm_tram_header->start = (u64)secondary_startup_64;
+
+	mm_tram_header->flags = 0;
+
+	// For now not needed anyways.
+	//mm_trampoline_pgd = (u64 *) __va(real_mode_header->trampoline_pgd);
+	//mm_trampoline_pgd[0] = trampoline_pgd_entry.pgd;
+
+	//for (i = pgd_index(__PAGE_OFFSET); i < PTRS_PER_PGD; i++)
+	//	mm_trampoline_pgd[i] = init_top_pgt[i].pgd;
+
+	/* This has to be set to 0 for the normal boot.
+	 * In case of reusing the trampoline from SMM,
+	 * this value will be overwritten by the driver.
+	 */
+
+	is_for_smm = 0;
+#endif
+
 	sme_sev_setup_real_mode(trampoline_header);
 }
 
@@ -184,7 +216,7 @@ static void __init setup_real_mode(void)
  */
 static void __init set_real_mode_permissions(void)
 {
-	unsigned char *base = (unsigned char *) real_mode_header;
+	unsigned char *base = (unsigned char *)real_mode_header;
 	size_t size = PAGE_ALIGN(real_mode_blob_end - real_mode_blob);
 
 	size_t ro_size =
@@ -196,11 +228,11 @@ static void __init set_real_mode_permissions(void)
 		real_mode_header->text_start;
 
 	unsigned long text_start =
-		(unsigned long) __va(real_mode_header->text_start);
+		(unsigned long)__va(real_mode_header->text_start);
 
-	set_memory_nx((unsigned long) base, size >> PAGE_SHIFT);
-	set_memory_ro((unsigned long) base, ro_size >> PAGE_SHIFT);
-	set_memory_x((unsigned long) text_start, text_size >> PAGE_SHIFT);
+	set_memory_nx((unsigned long)base, size >> PAGE_SHIFT);
+	set_memory_ro((unsigned long)base, ro_size >> PAGE_SHIFT);
+	set_memory_x((unsigned long)text_start, text_size >> PAGE_SHIFT);
 }
 
 void __init init_real_mode(void)
diff --git a/arch/x86/realmode/rm/Makefile b/arch/x86/realmode/rm/Makefile
index a0fb39abc5c8..9bd80a06a17b 100644
--- a/arch/x86/realmode/rm/Makefile
+++ b/arch/x86/realmode/rm/Makefile
@@ -23,6 +23,7 @@ realmode-y			+= header.o
 realmode-y			+= trampoline_$(BITS).o
 realmode-y			+= stack.o
 realmode-y			+= reboot.o
+realmode-$(CONFIG_SMM_DRIVER)	+= mm_trampoline.o
 realmode-$(CONFIG_ACPI_SLEEP)	+= $(wakeup-objs)
 
 targets	+= $(realmode-y)
diff --git a/arch/x86/realmode/rm/header.S b/arch/x86/realmode/rm/header.S
index 2eb62be6d256..683e91f2bfae 100644
--- a/arch/x86/realmode/rm/header.S
+++ b/arch/x86/realmode/rm/header.S
@@ -10,7 +10,7 @@
 #include <asm/segment.h>
 
 #include "realmode.h"
-	
+
 	.section ".header", "a"
 
 	.balign	16
@@ -20,6 +20,12 @@ SYM_DATA_START(real_mode_header)
 	/* SMP trampoline */
 	.long	pa_trampoline_start
 	.long	pa_trampoline_header
+#ifdef CONFIG_SMM_DRIVER
+	.long	pa_mm_startup_32
+	.long	pa_mm_trampoline_header
+	.long	pa_mm_trampoline_start64
+	.long	pa_mm_trampoline_pgd
+#endif
 #ifdef CONFIG_AMD_MEM_ENCRYPT
 	.long	pa_sev_es_trampoline_start
 #endif
diff --git a/arch/x86/realmode/rm/mm_trampoline.S b/arch/x86/realmode/rm/mm_trampoline.S
new file mode 100644
index 000000000000..7c44314b20f2
--- /dev/null
+++ b/arch/x86/realmode/rm/mm_trampoline.S
@@ -0,0 +1,242 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ *
+ *	Trampoline.S	Derived from Setup.S by Linus Torvalds
+ *
+ *	4 Jan 1997 Michael Chastain: changed to gnu as.
+ *	15 Sept 2005 Eric Biederman: 64bit PIC support
+ *
+ *	The purpose of this code is to get us from whatever mode
+ *	coreboot leaves us in when entering SMM, to the kernel space.
+ *
+ *	Derived from trampoline_$(BITS).S, shrinked to the parts that
+ *	are needed once in SMM (for e.g. we are at least in protected mode
+ *	at this point). The instructions from mm_startup_32 on are placed
+ *	by drivers/firmware/smm/mm_loader.c under designated SMRAM region.
+ *
+ *	Also see the general comment in trampoline_64.S.
+ */
+
+#include <linux/linkage.h>
+#include <asm/pgtable_types.h>
+#include <asm/page_types.h>
+#include <asm/msr.h>
+#include <asm/segment.h>
+#include <asm/processor-flags.h>
+#include <asm/realmode.h>
+
+#include "realmode.h"
+
+	.section ".text32","ax"
+	.code32
+	.balign 4
+SYM_CODE_START(mm_startup_32)
+	mov	$0x3f8, %dx
+	mov	$'H', %al
+	out	%al, (%dx)
+	mov	$'e', %al
+	out	%al, (%dx)
+	mov	$'l', %al
+	out	%al, (%dx)
+	mov	$'l', %al
+	out	%al, (%dx)
+	mov	$'0', %al
+	out	%al, (%dx)
+	mov	$' ', %al
+	out	%al, (%dx)
+	mov	$'f', %al
+	out	%al, (%dx)
+	mov	$'r', %al
+	out	%al, (%dx)
+	mov	$'o', %al
+	out	%al, (%dx)
+	mov	$'m', %al
+	out	%al, (%dx)
+	mov	$' ', %al
+	out	%al, (%dx)
+	mov	$'L', %al
+	out	%al, (%dx)
+	mov	$'i', %al
+	out	%al, (%dx)
+	mov	$'n', %al
+	out	%al, (%dx)
+	mov	$'u', %al
+	out	%al, (%dx)
+	mov	$'x', %al
+	out	%al, (%dx)
+	mov	$'\n', %al
+	out	%al, (%dx)
+
+	/*
+	 * For now, we return back to the coreboot's handler here.
+	 * TODO: preserve registers, and bring the CPU to the state
+	 * where the kernel address space is callable
+	 */
+	ret
+
+	// 0x80050033
+
+	lidtl	tr_idt	# load idt with 0, 0
+	lgdtl	tr_smm_gdt	# load gdt with whatever is appropriate
+
+	movw	$__KERNEL_DS, %dx	# Data segment descriptor
+
+	# Enable protected mode
+	movl	$(CR0_STATE & ~X86_CR0_PG), %eax
+	movl	%eax, %cr0		# into protected mode
+
+
+	movl	0x4010, %eax
+	movl	%eax, %cr4
+
+	movl	$0x3000, %eax
+	movl	%eax, %cr3
+	movl	$MSR_EFER, %ecx
+
+	//movl $0x80050033, %eax
+	//movl %eax, %cr0
+
+	/*
+	 * At this point we're in long mode but in 32bit compatibility mode
+	 * with EFER.LME = 1, CS.L = 0, CS.D = 1 (and in turn
+	 * EFER.LMA = 1). Now we want to jump in 64bit mode, to do that we use
+	 * the new gdt/idt that has __KERNEL_CS with CS.L = 1.
+	 */
+
+	ljmpl	$__KERNEL_CS, $pa_mm_startup_64
+SYM_CODE_END(mm_startup_32)
+
+	.section ".text64","ax"
+	.code64
+	.balign 4
+SYM_CODE_START(mm_startup_64)
+	jmpq *tr_smm_start(%rip)
+SYM_CODE_END(mm_startup_64)
+
+/*
+ * We start here if coreboot is compiled in 64-bit mode.
+ * Similarly as in mm_startup_32, for now we just return to
+ * the coreboot after acknowledging that we are here.
+ */
+SYM_CODE_START(mm_trampoline_start64)
+	mov	$0x3f8, %dx
+	mov	$'H', %al
+	out	%al, (%dx)
+	mov	$'e', %al
+	out	%al, (%dx)
+	mov	$'l', %al
+	out	%al, (%dx)
+	mov	$'l', %al
+	out	%al, (%dx)
+	mov	$'0', %al
+	out	%al, (%dx)
+	mov	$' ', %al
+	out	%al, (%dx)
+	mov	$'f', %al
+	out	%al, (%dx)
+	mov	$'r', %al
+	out	%al, (%dx)
+	mov	$'o', %al
+	out	%al, (%dx)
+	mov	$'m', %al
+	out	%al, (%dx)
+	mov	$' ', %al
+	out	%al, (%dx)
+	mov	$'L', %al
+	out	%al, (%dx)
+	mov	$'i', %al
+	out	%al, (%dx)
+	mov	$'n', %al
+	out	%al, (%dx)
+	mov	$'u', %al
+	out	%al, (%dx)
+	mov	$'x', %al
+	out	%al, (%dx)
+	mov	$'6', %al
+	out	%al, (%dx)
+	mov	$'4', %al
+	out	%al, (%dx)
+	mov	$'\n', %al
+	out	%al, (%dx)
+
+	ret
+	/*
+	 * APs start here on a direct transfer from 64-bit BIOS with identity
+	 * mapped page tables.  Load the kernel's GDT in order to gear down to
+	 * 32-bit mode (to handle 4-level vs. 5-level paging), and to (re)load
+	 * segment registers.  Load the zero IDT so any fault triggers a
+	 * shutdown instead of jumping back into BIOS.
+	 */
+	lidt	tr_idt(%rip)
+	lgdt	tr_smm_gdt64(%rip)
+
+	/* Check if paging mode has to be changed */
+	movq	%cr4, %rax
+	xorl	tr_smm_cr4(%rip), %eax
+	testl	$X86_CR4_LA57, %eax
+	jnz	.L_switch_paging
+
+	/* Paging mode is correct proceed in 64-bit mode */
+
+	//LOCK_AND_LOAD_REALMODE_ESP lock_rip=1
+
+	movw	$__KERNEL_DS, %dx
+	movl	%edx, %ss
+	addl	$pa_real_mode_base, %esp
+	movl	%edx, %ds
+	movl	%edx, %es
+	movl	%edx, %fs
+	movl	%edx, %gs
+
+	movl	$pa_mm_trampoline_pgd, %eax
+	movq	%rax, %cr3
+
+	pushq	$__KERNEL_CS
+	pushq	tr_smm_start(%rip)
+	lretq
+.L_switch_paging:
+	/*
+	 * To switch between 4- and 5-level paging modes, it is necessary
+	 * to disable paging. This must be done in the compatibility mode.
+	 */
+	ljmpl	*tr_smm_compat(%rip)
+SYM_CODE_END(mm_trampoline_start64)
+
+	.section ".rodata","a"
+	# Duplicate the global descriptor table
+	# so the kernel can live anywhere
+	.balign	16
+SYM_DATA_START(tr_smm_gdt)
+	.short	tr_smm_gdt_end - tr_smm_gdt - 1	# gdt limit
+	.long	pa_tr_smm_gdt
+	.short	0
+	.quad	0x00cf9b000000ffff	# __KERNEL32_CS
+	.quad	0x00af9b000000ffff	# __KERNEL_CS
+	.quad	0x00cf93000000ffff	# __KERNEL_DS
+SYM_DATA_END_LABEL(tr_smm_gdt, SYM_L_LOCAL, tr_smm_gdt_end)
+
+SYM_DATA_START(tr_smm_gdt64)
+	.short	tr_smm_gdt_end - tr_smm_gdt - 1	# gdt limit
+	.long	pa_tr_smm_gdt
+	.long	0
+SYM_DATA_END(tr_smm_gdt64)
+
+SYM_DATA_START(tr_smm_compat)
+	.long	0 //pa_mm_trampoline_compat
+	.short	__KERNEL32_CS
+SYM_DATA_END(tr_smm_compat)
+
+	.bss
+	.balign	PAGE_SIZE
+SYM_DATA(mm_trampoline_pgd, .space PAGE_SIZE)
+
+	.balign	8
+SYM_DATA_START(mm_trampoline_header)
+	SYM_DATA_LOCAL(tr_smm_start,	.space 8)
+	SYM_DATA(tr_smm_efer,		.space 8)
+	SYM_DATA(tr_smm_cr4,		.space 4)
+	SYM_DATA(tr_smm_flags,		.space 4)
+	SYM_DATA(tr_smm_lock,		.space 4)
+SYM_DATA_END(mm_trampoline_header)
+
+#include "trampoline_common.S"
diff --git a/drivers/firmware/google/Makefile b/drivers/firmware/google/Makefile
index 7ea48ec883f3..0a018f9c32f5 100644
--- a/drivers/firmware/google/Makefile
+++ b/drivers/firmware/google/Makefile
@@ -17,3 +17,4 @@ obj-$(CONFIG_GOOGLE_VPD)		+= vpd-sysfs.o
 obj-$(CONFIG_SMM_DRIVER)		+= smram.o
 obj-$(CONFIG_SMM_DRIVER)		+= mm_info.o
 obj-$(CONFIG_S3_SUPPORT_SMM)		+= s3_comm.o
+obj-$(CONFIG_SMM_DRIVER)		+= mm_loader.o
diff --git a/drivers/firmware/google/mm_loader.c b/drivers/firmware/google/mm_loader.c
new file mode 100644
index 000000000000..7007a4d604bc
--- /dev/null
+++ b/drivers/firmware/google/mm_loader.c
@@ -0,0 +1,247 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Driver for installing Linux-owned SMI handler
+ *
+ * Copyright (c) 2025 9elements GmbH
+ *
+ * Author: Michal Gorlas <michal.gorlas@9elements.com>
+ */
+
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/slab.h>
+#include <linux/cpu.h>
+#include <linux/delay.h>
+#include <asm/realmode.h>
+
+#include "smm.h"
+
+#define DRIVER_NAME "mm_loader"
+
+/*extern struct mm_info *mm_info;*/
+/*extern struct smram_info *smram;*/
+/*#ifdef CONFIG_S3_SUPPORT_SMM*/
+/*extern struct s3_comm_info *s3_info;*/
+/*#endif*/
+
+/* Getter for CBTABLE entries exposed by dedicated parsers.
+ * Should also free the memory allocated for the exposed structs.
+ */
+
+static struct smm_data get_cb_data(void)
+{
+	WARN_ON(!mm_info || !smram);
+#ifdef CONFIG_S3_SUPPORT
+	WARN_ON(!s3_info);
+#endif
+	int cpu_count;
+
+	cpu_count = num_possible_cpus();
+
+	struct smm_data ret = { .mm_info = *mm_info,
+				.nr_of_smm_regions = smram->nr_of_smm_regions,
+				.cpu_count = cpu_count,
+#ifdef CONFIG_S3_SUPPORT
+				.s3_info = *s3_info
+#endif
+	};
+
+	for (int i = 0; i < ret.nr_of_smm_regions; i++) {
+		ret.region[i].physical_start =
+			smram->descriptor[i].physical_start;
+		ret.region[i].cpu_start = smram->descriptor[i].cpu_start;
+		ret.region[i].physical_size =
+			smram->descriptor[i].physical_size;
+		ret.region[i].region_state = smram->descriptor[i].region_state;
+	}
+
+	// just so that the parser devices can be unmounted
+	kfree(mm_info);
+	kfree(smram);
+#ifdef CONFIG_S3_SUPPORT
+	kfree(s3_info);
+#endif
+
+	return ret;
+}
+
+static int trigger_smi(u64 cmd, u64 arg, u64 retry)
+{
+	// the values here have to be 64bit otherwise the compiler will cmplain
+	u64 status;
+	u16 apmc_port = 0xb2;
+
+	asm volatile("movq	%[cmd],  %%rax\n\t"
+		     "movq   %%rax,	%%rcx\n\t"
+		     "movq	%[arg],  %%rbx\n\t"
+		     "movq   %[retry],  %%r8\n\t"
+		     ".trigger:\n\t"
+		     "mov	%[apmc_port], %%dx\n\t"
+		     "outb	%%al, %%dx\n\t"
+		     "cmpq	%%rcx, %%rax\n\t"
+		     "jne .return_changed\n\t"
+		     "pushq  %%rcx\n\t"
+		     "movq   $10000, %%rcx\n\t"
+		     "rep    nop\n\t"
+		     "popq   %%rcx\n\t"
+		     "cmpq   $0, %%r8\n\t"
+		     "je     .return_not_changed\n\t"
+		     "decq   %%r8\n\t"
+		     "jmp    .trigger\n\t"
+		     ".return_changed:\n\t"
+// the dummy prints to the console should not be compiled for normal run - writing directly
+// to serial can (and usually will) cause some issues. Useful for debugging tho.
+#ifdef CONFIG_DEBUG_KERNEL
+		     "movw $0x3f8, %%dx\n\t"
+		     "movb $'c', %%al\n\t"
+		     "outb %%al, %%dx\n\t"
+		     "movb $'\n', %%al\n\t"
+		     "outb %%al, %%dx\n\t"
+#endif
+		     "movq	%%rax, %[status]\n\t"
+		     "jmp	.end\n\t"
+		     ".return_not_changed:"
+#ifdef CONFIG_DEBUG_KERNEL
+		     "movw $0x3f8, %%dx\n\t"
+		     "movb $'n', %%al\n\t"
+		     "outb %%al, %%dx\n\t"
+		     "movb $'\n', %%al\n\t"
+		     "outb %%al, %%dx\n\t"
+#endif
+		     "movq	%%rcx, %[status]\n\t"
+		     ".end:\n\t"
+		     : [status] "=r"(status)
+		     : [cmd] "r"(cmd), [arg] "r"(arg), [retry] "r"(retry),
+		       [apmc_port] "r"(apmc_port)
+		     : "%rax", "%rbx", "%rdx", "%rcx", "%r8");
+
+	// For debugging - it is useful to know what exact value was written to RAX by SMI handler.
+	printk(KERN_DEBUG "%s: SMI returned 0x%llx\n", __func__,
+	       ((status >> 8) & 0xff));
+
+	if (status == cmd)
+		status = PAYLOAD_MM_RET_FAILURE;
+	else
+		status = PAYLOAD_MM_RET_SUCCESS;
+
+	return status;
+}
+
+static int unlock_smram(struct smm_data *data)
+{
+	u64 cmd;
+	u8 status;
+
+	cmd = data->mm_info.register_mm_entry_swsmi |
+	      (PAYLOAD_MM_UNLOCK_SMRAM << 8);
+	status = trigger_smi(cmd, 0, 5);
+	pr_info(DRIVER_NAME ": %s: SMI returned %x\n", __func__, status);
+
+	return status;
+}
+
+static int lock_smram(struct smm_data *data)
+{
+	u64 cmd;
+	u8 status;
+
+	cmd = data->mm_info.register_mm_entry_swsmi |
+	      (PAYLOAD_MM_LOCK_SMRAM << 8);
+	status = trigger_smi(cmd, 0, 5);
+	pr_info(DRIVER_NAME ": %s: SMI returned %x\n", __func__, status);
+
+	return status;
+}
+
+static int register_entry_point(struct smm_data *data, uint32_t entry_point)
+{
+	u64 cmd;
+	u8 status;
+
+	cmd = data->mm_info.register_mm_entry_swsmi |
+	      (PAYLOAD_MM_REGISTER_ENTRY << 8);
+	status = trigger_smi(cmd, entry_point, 5);
+	pr_info(DRIVER_NAME ": %s: SMI returned %x\n", __func__, status);
+
+	return status;
+}
+
+static void notrace test(void *unused)
+{
+	pr_info("I am here\n");
+}
+
+static int __init mm_loader_init(void)
+{
+	struct smm_data cb_data = get_cb_data();
+	int status_unlock = 1;
+	int status_reg = 1;
+	int status_lock = 1;
+
+	is_for_smm = SMM_INIT_HANDLER;
+	ending_code = (unsigned long)test;
+	status_unlock = unlock_smram(&cb_data);
+
+	mdelay(100);
+	printk(KERN_DEBUG "status is %d\n", status_unlock);
+
+	/*
+	 * For now let the location be default SMBASE - this is not desired place,
+	 * but it is safe for now (till we have the address of the region translated properly),
+	 * we overwrite the stub code that is used for relocation only anyways.
+	 */
+	const uintptr_t location = 0x38000;
+
+	/*
+	 * Size is hardcoded to 3000: real_mode_blob without mm_trampoline.S is 6000, and
+	 * with is 9000, therefore since we want all instructions that are coming after
+	 * pa_mm_startup_32, we copy the real_mode_blob - mm_trampoline.S.
+	 */
+	void __iomem *addr = ioremap((resource_size_t)location, 3000);
+
+	/*
+	 * There are two things we have to do before telling coreboot where the handler is:
+	 * - modify is_for_smm so that head_$(BITS) will point to the handler code instead
+	 *   of continuing with the boot procedure (we do NOT want that to be done). I.e.
+	 *   it will call ending_code.
+	 * - let ending_code point to the handler code (for now to the dummy func).
+	 */
+	is_for_smm = SMM_INIT_HANDLER;
+	ending_code = (unsigned long)test;
+
+	/*
+	 * Depending on bitness of coreboot, we copy different entry code to SMRAM
+	 */
+	if (!cb_data.mm_info.requires_long_mode_call)
+		memcpy_toio(addr, __va(real_mode_header->mm_trampoline_start64),
+			    3000);
+	else
+		memcpy_toio(addr, __va(real_mode_header->mm_startup_32), 3000);
+
+	wbinvd();
+
+	status_reg = register_entry_point(&cb_data, location);
+
+	mdelay(100);
+	printk(KERN_DEBUG "status is %d\n", status_reg);
+
+	status_lock = lock_smram(&cb_data);
+
+	mdelay(100);
+	printk(KERN_DEBUG "status is %d\n", status_lock);
+
+	return 0;
+}
+
+static void __exit mm_loader_exit(void)
+{
+	// Nothing to be cleaned here.
+	printk(KERN_DEBUG "DONE");
+}
+
+module_init(mm_loader_init);
+module_exit(mm_loader_exit);
+
+MODULE_AUTHOR("Michal Gorlas <michal.gorlas@9elements.com>");
+MODULE_DESCRIPTION("MM loader - installs Linux-owned SMI handler");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/firmware/google/smm.h b/drivers/firmware/google/smm.h
index 3f0f63baefa0..5e5527e17a7a 100644
--- a/drivers/firmware/google/smm.h
+++ b/drivers/firmware/google/smm.h
@@ -11,6 +11,8 @@
 #ifndef __SMM_H
 #define __SMM_H
 
+#include "coreboot_table.h"
+
 #define PAYLOAD_MM_RET_SUCCESS 0
 #define PAYLOAD_MM_RET_FAILURE 1
 #define PAYLOAD_MM_UNLOCK_SMRAM 1
@@ -19,6 +21,11 @@
 
 extern u64 lshift(u64 opr, uint count);
 extern u64 unpack_cbuint64(struct cbuint64 inp);
+extern struct mm_info *mm_info;
+extern struct smram_info *smram;
+#ifdef CONFIG_S3_SUPPORT_SMM
+extern struct s3_comm_info *s3_info;
+#endif
 
 struct generic_register {
 	u8 register_id;
-- 
2.49.0

